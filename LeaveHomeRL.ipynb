{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZyxPqfCNGn8"
      },
      "source": [
        "# Environment\n",
        "## States\n",
        "There are five rooms and two areas in which the robot could enter. Each of them is a state.\n",
        "\n",
        "## Actions\n",
        "From every room, there are several doors that would connect the room to another room or area. The robot's action is to choose one of these doors and go through it. Note that some doors have only one room, and areas could be accessed via one door that connects them to a room.\n",
        "\n",
        "## Rewards\n",
        "Area numbered 5 has a reward of 10, and Area numbered 6 has a penalty of -10. Every action (entering rooms) has a penalty of -1.\n",
        "\n",
        "## Goal State\n",
        "The goal state is area number 5. The robot is supposed to start from room number 2 and reach area number 5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki7PLVfgQ2oI"
      },
      "source": [
        "# Environment Implementation\n",
        "We use an adjacency list for representing the graph and a list of dictionaries for Q, as you can see here. Rewards (and penalties) are stored in an adjacency matrix, and edge weights represent the rewards. Also, the initial value for Q is zero.\n",
        "\n",
        "# Learning\n",
        "This part is implemented by two loops. The first is for episodes that would run the games `n_episodes` times. The second is responsible for one game and would end if the agent ends up in a trap or goal or the number of steps exceeds a specific number (Here, it is 1000).\n",
        "\n",
        "In each step, first, we would choose an action for the agent based on its state. This decision is made based on `epsilon`, the probability of making a random decision, and if it does not make an arbitrary decision, it will choose the best action (node). This algorithm is called *Epsilon-greedy*.\n",
        "\n",
        "After selecting the next node, we would update Q based on Q learning formula (specified in `calculate_new_Q`). Then we change the agent state and repeat these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "NwAxWj0YR88P",
        "outputId": "e2d39de5-09c8-478a-da2d-1dfde93f81eb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class Graph:\n",
        "\tdef __init__(self, adj, goals, traps, n_episods, learning_rate, discount_factor, epsilon, goal_reward, trap_penalty, movement_penalty):\n",
        "\t\tself.adj = adj\n",
        "\t\tself.learning_rate = learning_rate\n",
        "\t\tself.discount_factor = discount_factor\n",
        "\t\tself.epsilon = epsilon\n",
        "\t\tself.n_episods = n_episods\n",
        "\t\tself.goals = goals\n",
        "\t\tself.traps = traps\n",
        "\n",
        "\t\tself.rewards = []\n",
        "\t\tfor i in range(len(self.adj)):\n",
        "\t\t\tself.rewards.append([])\n",
        "\t\t\tfor j in range(len(self.adj)):\n",
        "\t\t\t\tif j in self.adj[i]:\n",
        "\t\t\t\t\tif j in goals:\n",
        "\t\t\t\t\t\tself.rewards[-1].append(goal_reward)\n",
        "\t\t\t\t\telif j in traps:\n",
        "\t\t\t\t\t\tself.rewards[-1].append(trap_penalty)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tself.rewards[-1].append(movement_penalty)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tself.rewards[-1].append(0)\n",
        "\n",
        "\t\tself.Q = []\n",
        "\t\tfor node1 in self.adj:\n",
        "\t\t\tself.Q.append(dict())\n",
        "\t\t\tfor node2 in node1:\n",
        "\t\t\t\tself.Q[-1][node2] = 0\n",
        "\n",
        "\tdef learn(self, first_node):\n",
        "\t\tfor _ in range(self.n_episods):\n",
        "\t\t\tstep = 0\n",
        "\t\t\tcurrent_node = first_node\n",
        "\t\t\twhile step < 10000 and current_node not in self.goals and current_node not in self.traps:\n",
        "\t\t\t\tnext_node = self.choose_action(current_node)\n",
        "\t\t\t\tself.calculate_new_Q(current_node, next_node)\n",
        "\t\t\t\tcurrent_node = next_node\n",
        "\t\t\t\tstep += 1\n",
        "\n",
        "\tdef calculate_new_Q(self, current_node, next_node):\n",
        "\t\tself.Q[current_node][next_node] = (1 - self.learning_rate) * self.Q[current_node][next_node] + \\\n",
        "\t\t\tself.learning_rate * \\\n",
        "\t\t\t(self.rewards[current_node][next_node] +\n",
        "\t\t\t self.discount_factor * max(self.Q[next_node].values()))\n",
        "\n",
        "\tdef choose_action(self, current_node):\n",
        "\t\tif random.random() < self.epsilon:\n",
        "\t\t\treturn random.choice(self.adj[current_node])\n",
        "\t\telse:\n",
        "\t\t    return random.choice([key for key in self.Q[current_node]\n",
        "                        if self.Q[current_node][key] == max(self.Q[current_node].values())])\n",
        "\n",
        "\tdef play(self, first_node):\n",
        "\t\tstep = 0\n",
        "\t\tcurrent_node = first_node\n",
        "\t\twhile step < 10 and current_node not in self.goals and current_node not in self.traps:\n",
        "\t\t\tprint('Step:', step, '\\tNode:', current_node)\n",
        "\t\t\tnext_node = random.choice([key for key in self.Q[current_node]\n",
        "                              if self.Q[current_node][key] == max(self.Q[current_node].values())])\n",
        "\t\t\tcurrent_node = next_node\n",
        "\t\t\tstep += 1\n",
        "\t\tprint('Step:', step, '\\tNode:', current_node)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing\n",
        "This part the agent would play once and only choose the best action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "dj = [\n",
        "\t[4],\n",
        "\t[3, 5],\n",
        "\t[3],\n",
        "\t[1, 2, 4],\n",
        "\t[0, 3, 6],\n",
        "\t[1],\n",
        "\t[4]\n",
        "]\n",
        "\n",
        "goals = [5]\n",
        "traps = [6]\n",
        "\n",
        "goal_reward = 10\n",
        "trap_penalty = -10\n",
        "\n",
        "movement_penalty = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we test the algorithm with 5 different parameters\n",
        "\n",
        "## First\n",
        "As you could see it found the best path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, {4: -0.8}), (1, {3: 4.52679261093888, 5: 9.999872}), (2, {3: 4.59009217143505}), (3, {1: 6.9994054254592, 2: 2.6526596108721856, 4: -1.6348160000000003}), (4, {0: -0.8, 3: -1.7497600000000002, 6: -9.92}), (5, {1: 0}), (6, {4: 0})]\n",
            "Step: 0 \tNode: 2\n",
            "Step: 1 \tNode: 3\n",
            "Step: 2 \tNode: 1\n",
            "Step: 3 \tNode: 5\n"
          ]
        }
      ],
      "source": [
        "n_episods = 10\n",
        "\n",
        "learning_rate = 0.8\n",
        "\n",
        "discount_factor = 0.8\n",
        "\n",
        "epsilon = 0.8\n",
        "\n",
        "graph = Graph(adj, goals, traps, n_episods, learning_rate, discount_factor,\n",
        "              epsilon, goal_reward, trap_penalty, movement_penalty)\n",
        "\n",
        "graph.learn(2)\n",
        "print(list(enumerate(graph.Q)))\n",
        "graph.play(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second\n",
        "As you could see it found the best path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, {4: -0.9}), (1, {3: 0, 5: 9.9}), (2, {3: -1.4048999999999998}), (3, {1: 3.06, 2: -1.3455000000000001, 4: -0.99}), (4, {0: -0.9, 3: -1.305, 6: -9.0}), (5, {1: 0}), (6, {4: 0})]\n",
            "Step: 0 \tNode: 2\n",
            "Step: 1 \tNode: 3\n",
            "Step: 2 \tNode: 1\n",
            "Step: 3 \tNode: 5\n"
          ]
        }
      ],
      "source": [
        "n_episods = 3\n",
        "\n",
        "learning_rate = 0.9\n",
        "\n",
        "discount_factor = 0.5\n",
        "\n",
        "epsilon = 0.5\n",
        "\n",
        "graph = Graph(adj, goals, traps, n_episods, learning_rate, discount_factor,\n",
        "              epsilon, goal_reward, trap_penalty, movement_penalty)\n",
        "\n",
        "graph.learn(2)\n",
        "print(list(enumerate(graph.Q)))\n",
        "graph.play(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Third\n",
        "Here as you could see because of low learning rate, low discount factor the agent was unable to learn what action are good because the rewards had very little effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, {4: 0}), (1, {3: 0, 5: 1.9}), (2, {3: -0.3458}), (3, {1: -0.18000000000000002, 2: -0.10189999999999999, 4: -0.19}), (4, {0: 0, 3: -0.1, 6: -1.0}), (5, {1: 0}), (6, {4: 0})]\n",
            "Step: 0 \tNode: 2\n",
            "Step: 1 \tNode: 3\n",
            "Step: 2 \tNode: 2\n",
            "Step: 3 \tNode: 3\n",
            "Step: 4 \tNode: 2\n",
            "Step: 5 \tNode: 3\n",
            "Step: 6 \tNode: 2\n",
            "Step: 7 \tNode: 3\n",
            "Step: 8 \tNode: 2\n",
            "Step: 9 \tNode: 3\n",
            "Step: 10 \tNode: 2\n"
          ]
        }
      ],
      "source": [
        "n_episods = 3\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "discount_factor = 0.1\n",
        "\n",
        "epsilon = 0.8\n",
        "\n",
        "graph = Graph(adj, goals, traps, n_episods, learning_rate, discount_factor,\n",
        "              epsilon, goal_reward, trap_penalty, movement_penalty)\n",
        "\n",
        "graph.learn(2)\n",
        "print(list(enumerate(graph.Q)))\n",
        "graph.play(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forth\n",
        "Here as you could see even though learning rate and discount factor is low because number of episodes is relatively high, the agent had enough chance to explore the environment and experience different action enough to find the best path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, {4: -0.27626619}), (1, {3: -0.42430913740000004, 5: 3.439}), (2, {3: -0.8177032694358313}), (3, {1: -0.4856000220000002, 2: -0.546942358302851, 4: -0.5269035900000001}), (4, {0: -0.273837101, 3: -0.27371, 6: -3.439}), (5, {1: 0}), (6, {4: 0})]\n",
            "Step: 0 \tNode: 2\n",
            "Step: 1 \tNode: 3\n",
            "Step: 2 \tNode: 1\n",
            "Step: 3 \tNode: 5\n"
          ]
        }
      ],
      "source": [
        "n_episods = 8\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "discount_factor = 0.1\n",
        "\n",
        "epsilon = 0.8\n",
        "\n",
        "graph = Graph(adj, goals, traps, n_episods, learning_rate, discount_factor,\n",
        "              epsilon, goal_reward, trap_penalty, movement_penalty)\n",
        "\n",
        "graph.learn(2)\n",
        "print(list(enumerate(graph.Q)))\n",
        "graph.play(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fifth\n",
        "In this example because the epsilon is very low we don't give a chance to the agent to explore the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, {4: -0.34921769933579005}), (1, {3: 0, 5: 5.217031}), (2, {3: -0.7329255530701002}), (3, {1: -0.3720087000000001, 2: -0.3595746399790001, 4: -0.41477710100000004}), (4, {0: -0.34916746582429004, 3: -0.35229170347210004, 6: -1.0}), (5, {1: 0}), (6, {4: 0})]\n",
            "Step: 0 \tNode: 2\n",
            "Step: 1 \tNode: 3\n",
            "Step: 2 \tNode: 2\n",
            "Step: 3 \tNode: 3\n",
            "Step: 4 \tNode: 2\n",
            "Step: 5 \tNode: 3\n",
            "Step: 6 \tNode: 2\n",
            "Step: 7 \tNode: 3\n",
            "Step: 8 \tNode: 2\n",
            "Step: 9 \tNode: 3\n",
            "Step: 10 \tNode: 2\n"
          ]
        }
      ],
      "source": [
        "n_episods = 8\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "discount_factor = 0.1\n",
        "\n",
        "epsilon = 0.01\n",
        "\n",
        "graph = Graph(adj, goals, traps, n_episods, learning_rate, discount_factor,\n",
        "              epsilon, goal_reward, trap_penalty, movement_penalty)\n",
        "\n",
        "graph.learn(2)\n",
        "print(list(enumerate(graph.Q)))\n",
        "graph.play(2)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LeaveHomeRL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
